{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLI Metric Evaluation\n",
    "\n",
    "- New **focus** of the paper comes from a finding of the thesis:\n",
    "\n",
    "  - \"The proposed _contradiction rate metric_ turns out to be correlated with human\n",
    "    judgement, so it is a good indicator of human prediction consistency and allows it to be assessed automatically without the need for annotations requiring significant human effort.\"\n",
    "  - We would understand how entailment is correlated with human evaluation\n",
    "\n",
    "- Example of the assessment:\n",
    "  - Consider a pair of strings (input and adversarial example)\n",
    "    - eg. [\"it's a very <font color = green>valuable</font> film . . .\", \"it's a very <font color = red>inestimable</font> film . . .\"]\n",
    "  - We use the human annotations collected for the master's thesis as the _gold annotations_ (ignoring UNCLEAR annotations)\n",
    "  - Then compute the automatic annotations using a pre-trained [NLI model](https://huggingface.co/cross-encoder/nli-deberta-v3-base) considering as:\n",
    "    - INCONSISTENT if the model predicts CONTRADICTION (considering both directions, A->B and A<-B)\n",
    "    - CONSISTENT in all other cases\n",
    "  - Use [Cohenâ€™s kappa](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html) as statistic to measures inter-annotator agreement (0.6+ is good)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnnotationEvaluator:\n",
    "    def __init__(self, \n",
    "                 annotation_file_path: str,\n",
    "                 annotation_to_idx: dict,\n",
    "                 nli_model: str = \"cross-encoder/nli-deberta-v3-base\",\n",
    "                 attack_col: str = \"Attack\",\n",
    "                 annotation_col: str = \"Annotation\",\n",
    "                 remove_unclear: bool = True\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Provides a simple interface to perform an automated evaluation of the attack and compare it with the gold annotations.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        annotation_file_path: str\n",
    "            Path to the gold annotation file. The file should be a csv file with two columns: \"Attack\" and \"Annotation\".\n",
    "        annotation_to_idx: dict\n",
    "            The labels that are used for the annotation. The labels should be strings and match the labels in the gold annotation file.\n",
    "        nli_model: str\n",
    "            The name of the NLI model to use for the evaluation. The model should be a cross-encoder model.\n",
    "        attack_col: str\n",
    "            The name of the column in the gold annotation file that contains the attack.\n",
    "        annotation_col: str\n",
    "            The name of the column in the gold annotation file that contains the gold annotation.\n",
    "        remove_unclear: bool\n",
    "            If True, the unclear annotations are removed from the gold annotations, since they cannot be predicted by the NLI model.\n",
    "        \"\"\"\n",
    "        self.annotation_to_idx = annotation_to_idx\n",
    "        self.attack_col = attack_col\n",
    "        self.annotation_col = annotation_col\n",
    "        self.nli_model = CrossEncoder(nli_model)\n",
    "        self.nli_label_to_idx = {\"contradiction\": 0, \"entailment\": 1, \"neutral\": 2}\n",
    "\n",
    "        self.gold_annotations = pd.read_csv(annotation_file_path)\n",
    "        self.gold_annotations = self.__process_gold_annotations()\n",
    "        if remove_unclear:\n",
    "            self.gold_annotations = self.gold_annotations[self.gold_annotations[\"gold_annotation\"] != self.annotation_to_idx[\"UNCLEAR\"]]\n",
    "\n",
    "\n",
    "    def __separate_original_perturbed(self, text: str):\n",
    "        \"\"\"\n",
    "        Splits the text into original and perturbed sentence.\n",
    "        \"\"\"\n",
    "        text = re.sub(\"<.*?>\", \"\", text) # remove html tags\n",
    "        \n",
    "        # Split text into original and perturbed \n",
    "        original, perturbed = text.split(\"Perturbed:\")\n",
    "        original = original.split(\"Original:\")[1].strip()\n",
    "        perturbed = perturbed.strip()\n",
    "        return original, perturbed\n",
    "\n",
    "    def __process_gold_annotations(self):\n",
    "        \"\"\"\n",
    "        Processes the gold annotations and creates a datafra,e with the original and perturbed sentence separated and the gold annotation as idx.\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame(columns=[\"original\", \"perturbed\"])\n",
    "\n",
    "        df['original'], df['perturbed'] = zip(*self.gold_annotations[self.attack_col].map(self.__separate_original_perturbed))\n",
    "        df['gold_annotation'] = self.gold_annotations[self.annotation_col].map(self.annotation_to_idx)\n",
    "        return df\n",
    "    \n",
    "    def get_gold_annotations(self):\n",
    "        \"\"\"\n",
    "        Returns the list gold annotations.\n",
    "        \"\"\"\n",
    "        return self.gold_annotations[\"gold_annotation\"].tolist()\n",
    "    \n",
    "    def __preds_to_annotations(self, preds, preds_reverse):\n",
    "        \"\"\"\n",
    "        Converts the NLI predictions to annotation labels.\n",
    "        A prediction is considered inconsistent if the nli model predicts \"contradiction\" for one of the two directions of entailment.\n",
    "        In all other cases the prediction is considered consistent.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        preds: list\n",
    "            The predictions of the NLI model for (original sentence -> perturbed sentence)\n",
    "        preds_reverse: list\n",
    "            The predictions of the NLI model for (perturbed sentence -> original sentence)\n",
    "        \"\"\"\n",
    "        nli_annotations = []\n",
    "        for pred, pred_reverse in zip(preds, preds_reverse):\n",
    "            if (pred == self.nli_label_to_idx[\"contradiction\"]) or (pred_reverse == self.nli_label_to_idx[\"contradiction\"]):\n",
    "                nli_annotations.append(self.annotation_to_idx[\"INCONSISTENT\"])\n",
    "            else:\n",
    "                nli_annotations.append(self.annotation_to_idx[\"CONSISTENT\"])\n",
    "        return nli_annotations\n",
    "\n",
    "    def get_nli_annotations(self):\n",
    "        \"\"\"\n",
    "        Returns the NLI automatic annotations\n",
    "        \"\"\"\n",
    "        inputs = list(self.gold_annotations[['original', 'perturbed']].itertuples(index=False, name=None))\n",
    "        nli_scores = self.nli_model.predict(inputs)\n",
    "        nli_preds = nli_scores.argmax(axis=1)\n",
    "\n",
    "        inputs_reverse = list(self.gold_annotations[['perturbed', 'original']].itertuples(index=False, name=None))\n",
    "        nli_scores_reverse = self.nli_model.predict(inputs_reverse)\n",
    "        nli_preds_reverse = nli_scores_reverse.argmax(axis=1)\n",
    "\n",
    "        return self.__preds_to_annotations(nli_preds, nli_preds_reverse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_paths = list(Path('annotations/thesis_gold/').glob('*.csv'))\n",
    "\n",
    "annotation_to_idx = {\n",
    "    \"CONSISTENT\" : 0,\n",
    "    \"INCONSISTENT\" : 1,\n",
    "    \"UNCLEAR\" : 2,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff9aaf3920a0447b957bd06262a6cc13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gold_annotations = []\n",
    "nli_annotations = []\n",
    "\n",
    "for annotation_path in tqdm(annotation_paths):\n",
    "    evaluator = AnnotationEvaluator(annotation_path, annotation_to_idx, remove_unclear=True)\n",
    "\n",
    "    gold_annotations += evaluator.get_gold_annotations()\n",
    "    nli_annotations += evaluator.get_nli_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total annotations processed: 264\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total annotations processed: {len(gold_annotations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's kappa: 0.6050269299820467\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cohen's kappa: {cohen_kappa_score(gold_annotations, nli_annotations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong predictions: 40\n",
      "Inconsistent predicted as Consistent: 7\n",
      "Consistent predicted as Inconsistent: 33\n"
     ]
    }
   ],
   "source": [
    "gold_annotations = pd.Series(gold_annotations)\n",
    "nli_annotations = pd.Series(nli_annotations)\n",
    "idx_diff = gold_annotations != nli_annotations\n",
    "\n",
    "wrong_predictions = sum(idx_diff)\n",
    "inconsistent_consistent = sum((gold_annotations[idx_diff] == 1) & (nli_annotations[idx_diff] == 0))\n",
    "consistent_inconsistent = sum((gold_annotations[idx_diff] == 0) & (nli_annotations[idx_diff] == 1))\n",
    "\n",
    "print(f\"Wrong predictions: {wrong_predictions}\")\n",
    "print(f\"Inconsistent predicted as Consistent: {inconsistent_consistent}\")\n",
    "print(f\"Consistent predicted as Inconsistent: {consistent_inconsistent}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3rdplace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "74395c6fa573f9609fbdb613d31c3b5e5bf3bb88cc4a59c4ca3c6b11483c6e9b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
